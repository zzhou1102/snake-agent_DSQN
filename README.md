# Snake AI Project
该项目展示了一个基于Deep Q-Learning的AI智能体，该智能体可以玩经典的贪吃蛇游戏。该智能体通过神经网络和强化学习技术进行训练，以提高其自主玩游戏的能力。

## 1.项目结构

该项目主要由以下几个部分组成：

- `agent.py`: 包含负责与游戏环境交互并训练模型的智能体。
- `game.py`: 包含贪吃蛇游戏的实现，包括游戏机制、蛇的移动和食物的生成。
- `main.py`: 运行游戏和训练过程，使智能体能够通过玩多局游戏来学习。
- `model.py`: 定义了神经网络模型（Linear_QNet）以及用于训练该模型的QTrainer类。
- `settings.py`: 存储游戏、模型和训练配置的各种设置和超参数。
- `par_lev.json`: 一个配置文件，其中包含训练过程中使用的不同级别或环境的参数。
- `test.py` (由 `test_snake_ai` 代码生成): 一个通过运行Snake游戏并观察AI的表现来测试已训练模型的脚本。

## 2.项目实现

### 2.1项目工具

- Python 3.x
- PyTorch
- Pygame
- NumPy

你可以使用以下命令安装所需的Python包：
使用pip安装torch、pygame和numpy
```sh
pip install torch pygame numpy
```

### 2.2训练模型

要训练模型，请运行 `main.py` 脚本. 这将使用Deep Q-Learning来训练Snake AI，使智能体能够在多次迭代中边玩边学。训练参数，如学习率和游戏次数，可以在 `par_lev.json` 文件中进行调整。

```sh
python main.py
```

### 2.3测试模型

要测试训练好的模型并查看AI玩游戏的效果，请使用 `test.py` 脚本. 该脚本从 `model` 该脚本从模型文件夹中加载模型并运行游戏循环，从而展示AI玩游戏的能力。

```sh
python test.py
```

### 2.4文件描述

- **`agent.py`**: 实现了一个使用神经网络来决定最佳行动的AI智能体。该智能体通过存储游戏状态、行动、奖励，并利用这些经验来训练模型，从而进行学习。
- **`game.py`**: 包含贪吃蛇游戏的实现。它包括处理游戏逻辑的函数，如移动蛇、检测碰撞和生成食物。
- **`main.py`**: 设置游戏和智能体，并运行训练循环。这是智能体通过与环境交互学习玩游戏的地方。
- **`model.py`**: 定义了神经网络 (`Linear_QNet`) 该网络预测智能体应采取的最佳行动。此外，它还包含 `QTrainer`类，用于处理模型的训练。
- **`settings.py`**: 存储在整个项目中使用的常量与超参数，如屏幕大小、蛇移动速度、学习率、epsilon等。
- **`par_lev.json`**: 存储训练过程中使用的不同游戏级别或环境的配置参数。

## 3.工作原理

Snake AI利用神经网络根据游戏当前状态预测最佳行动。该智能体通过强化学习进行训练，特别是Deep Q-Learning，它通过玩游戏来学习，对积极行动（例如，吃食物）给予奖励，对消极行动（例如，撞到墙壁或自己）给予惩罚。

### 3.1主要特点

- **Deep Q-Learning**: 智能体使用带有神经网络 (Deep Q-Network)的Q-Learning来近似Q值函数。
- **探索 vs. 利用**: 智能体采用ε-贪婪策略，在探索新动作和利用已知动作之间进行权衡。
- **奖励系统**: 智能体因进食而获得奖励，因碰撞或陷入循环而受到惩罚，这激励它学习有效的策略。

## 4.未来改进

- **高级神经网络**:尝试使用更复杂的架构，如卷积神经网络（CNN），以提高学习效率。
- **更精细的奖励系统**: 实施更精细的奖励系统，以鼓励采用更优的长期策略。
- **多智能体系统**: 引入多个智能体，观察它们是否能学习协作或竞争策略。

## 5.致谢

- **PyTorch** 提供深度学习框架。
- **Pygame** 为游戏开发提供了一个易于使用的库。
- 从各种强化学习教程和贪吃蛇游戏实现中获得灵感。

## 6.联系方式

如果您有任何疑问或建议，请随时与我联系，或在项目仓库中提交问题。

## 7.成员分工

- **成昊阳**：构思项目场景、整体项目的架构、负责将整体进行转化为SNN并完成测试性能实验
- **周震**：参与各模块核心程序编写、项目说明底稿

